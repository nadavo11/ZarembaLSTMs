{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"yKfoaUcQVwyh"},"source":["# LSTM for text prediction"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"FHLtSrk1Vwym"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:55.906369Z","start_time":"2024-07-16T22:07:55.739112Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1721218613983,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"rvTqUjtLVwyn","outputId":"3ef4e01f-6237-4088-d454-6bacdc3f5997"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from datetime import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","# from torchtext.data.utils import get_tokenizer\n","# from torchtext.vocab import build_vocab_from_iterator\n","# import torchtext.transforms as T\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17173,"status":"ok","timestamp":1721218773576,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"ktPkPCqDnJNm","outputId":"95524a20-b46e-49d6-ba23-ce1668ced5f2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1721220009127,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"esTuBCbsnKTS"},"outputs":[],"source":["FOLDER_PATH = '/content/drive/MyDrive/Deep Learning/ex2/PTB'\n","if (os.path.exists(FOLDER_PATH)):\n","  path = FOLDER_PATH\n","else:\n","  path = \"data\" #for git runs"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.024515Z","start_time":"2024-07-16T22:07:55.921645Z"},"id":"uHcLUeYaVwyr"},"outputs":[],"source":["#hyper parameters\n","batch_size = 20\n","seq_length = 35\n","lr = 1e-3\n","criterion = nn.CrossEntropyLoss()\n","epochs = 15\n","hidden_size =200\n","input_size = 200\n","num_layers =  2"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"mIWhdAqXVwys"},"source":["# Tensorboard\n","tensorboard is a visualization tool that can be used to visualize the training process of a deep learning model. The torch.utils.tensorboard.SummaryWriter class is used to write the logs to the tensorboard. The SummaryWriter class takes the log directory as input. The logs are written to the log directory in the form of event files. The event files can be visualized using the tensorboard web interface."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.059718Z","start_time":"2024-07-16T22:07:56.031483Z"},"id":"4eZVqvjwVwyt"},"outputs":[],"source":["#%tensorboard --logdir runs\n","log_dir = os.path.join(os.getcwd(), \"runs\")\n","os.makedirs(log_dir, exist_ok=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XHIaOvfCvE2"},"outputs":[],"source":["!pip install tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78900,"status":"ok","timestamp":1721217548604,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"yQNkD2saCE2e","outputId":"96a3e46c-54be-4e5e-e64d-eaaf56454557"},"outputs":[],"source":["!pip install torch torchvision\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujkbxP5PDb85"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1BU_SIgDRqj"},"outputs":[],"source":["!tensorboard --logdir=runs"]},{"cell_type":"markdown","metadata":{"id":"TIoDoKLwCEmZ"},"source":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"GGK2TDyOVwyu"},"source":["# Data Preprocessing\n","we will train the LSTM model on the Penn  Treebank dataset. The Penn Treebank dataset is a dataset of cleaned and annotated English text. The data is split into training, validation, and testing sets."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"s_n1qR6SVwyu"},"source":["## load train, test, and validation data"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.139287Z","start_time":"2024-07-16T22:07:56.068212Z"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1721220021990,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"DKvQOHyGVwyv"},"outputs":[],"source":["train_data_raw = open(f'{path}/ptb.train.txt', 'r').read()\n","test_data_raw = open(f'{path}/ptb.test.txt', 'r').read()\n","valid_data_raw = open(f'{path}/ptb.valid.txt', 'r').read()\n","data =  train_data_raw + ' ' + test_data_raw + ' ' + valid_data_raw\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"UPlkR_k0Vwyv"},"source":["### sample from the data\n"," let's see what are the most common words in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.321556Z","start_time":"2024-07-16T22:07:56.302793Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1721218835312,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"yg-m69t8Vwyw","outputId":"6a95a9e8-661e-49c3-8a7e-0c5cef6528ac"},"outputs":[],"source":["from collections import Counter\n","\n","leaderboard = Counter(data.split()).most_common(10)\n","i = 0\n","for word, freq in leaderboard:\n","    i+=1\n","    print(f'{i}.{word}: appears {freq} times')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"ODiGCzB6Vwyx"},"source":["## Tokenizing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.340068Z","start_time":"2024-07-16T22:07:56.316608Z"},"id":"AdHTnmbXVwyx"},"outputs":[],"source":["# Tokenize the data\n","def tokenize(text):\n","    return text.replace('\\n', '<eos>').split()\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"r_5MwZGWVwyx"},"source":["### create a vocabulary of words\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.588812Z","start_time":"2024-07-16T22:07:56.333996Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":804,"status":"ok","timestamp":1721218841234,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"6PEPce7QVwyy","outputId":"c3de3360-cc8a-4528-998f-e72fb3434767"},"outputs":[],"source":["def build_vocab(text):\n","    tokens = tokenize(text)\n","    counter = Counter(tokens)\n","    vocab = sorted(counter, key=counter.get, reverse=True)\n","    vocab = {word: i for i, word in enumerate(vocab, 1)}\n","\n","    return vocab\n","\n","\n","vocab = build_vocab(data)\n","vocab_size = len(vocab) + 1\n","\n","\n","\n","print(f'vocab size: {vocab_size}')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.678616Z","start_time":"2024-07-16T22:07:56.618538Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"77gi0AjkVwyz","outputId":"7c7c09dd-6dbc-4971-8f5d-77f65452cd23"},"outputs":[],"source":["vocab"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"_LEkTL9gVwyz"},"source":["### decode and encode the words\n"," let's create a function that converts a word to token index and vice versa: the function stoi converts a word to a token index and the function itos converts a token index to a word"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.679621Z","start_time":"2024-07-16T22:07:56.644605Z"},"id":"55_OOOoaVwyz"},"outputs":[],"source":["\n","# decode the token i to a word S\n","def itos(i):\n","    return list(vocab.keys())[i-1]\n","\n","# encode the word S to a token index i\n","def stoi(s):\n","    return vocab[s] if s in vocab else vocab['<unk>']"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:56.680620Z","start_time":"2024-07-16T22:07:56.660663Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":899,"status":"ok","timestamp":1721218900549,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"IuUKIIf4Vwy0","outputId":"83a296cb-11b6-4682-d3c6-f2db921c4fa7"},"outputs":[],"source":["print(vocab)\n","\n","print(stoi('year'))\n","print(itos(42))\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"Kb0zl2cuVwy0"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.121018Z","start_time":"2024-07-16T22:07:56.836019Z"},"id":"sPBrYPS1Vwy0"},"outputs":[],"source":["train_data = [stoi(word) for word in train_data_raw.split()]\n","valid_data = [stoi(word) for word in valid_data_raw.split()]\n","test_data = [stoi(word) for word in test_data_raw.split()]"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"IkBqJ4sFVwy1"},"source":["## build a dataset and dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.187641Z","start_time":"2024-07-16T22:07:57.145054Z"},"id":"1MnIoCzZVwy1"},"outputs":[],"source":["class PTBDataset(Dataset):\n","    def __init__(self, data, seq_length):\n","        self.data = data\n","        self.seq_length = seq_length\n","\n","    def __len__(self):\n","        return len(self.data) // self.seq_length\n","\n","    def __getitem__(self, idx):\n","        x = self.data[idx * self.seq_length: (idx + 1) * self.seq_length]\n","        y = self.data[idx * self.seq_length + 1: (idx + 1) * self.seq_length + 1]\n","        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n","\n","\n","# Create datasets\n","train_dataset = PTBDataset(train_data, seq_length)\n","valid_dataset = PTBDataset(valid_data, seq_length)\n","test_dataset = PTBDataset(test_data, seq_length)\n","\n","# Create dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","test_data_iter = iter(test_loader)\n","train_data_iter = iter(train_loader)\n","valid_data_iter = iter(valid_loader)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"2KEhHH7pVwy2"},"source":["### display a batch of data\n","note that y is exactly x shifted by one position. meaning that $y_i = x_{i+1} = LSTM(x[0:i-1])$"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.198126Z","start_time":"2024-07-16T22:07:57.168292Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1721218917604,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"RRSaW_xCVwy2","outputId":"881b6ce2-e376-453d-ffa4-8305b9aa2170"},"outputs":[],"source":["x, y = next(train_data_iter)\n","print(f'x: {x.size()}, y: {y.size()}')\n","print(f'x:{\" \".join([itos(i) for i in x[0]])}')\n","print(f'y:{\" \".join([itos(i) for i in y[0]])}')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"90K1jBySVwy2"},"source":["# LSTM/GRU Cell definition\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.239903Z","start_time":"2024-07-16T22:07:57.208089Z"},"id":"PHyIH_gDVwy3"},"outputs":[],"source":["class GRU_Cell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.reset_gate = nn.Sequential(nn.Linear(input_size + hidden_size, hidden_size),nn.Sigmoid() )\n","        self.update_gate = nn.Sequential(nn.Linear(input_size + hidden_size, hidden_size),nn.Sigmoid() )\n","        self.new_gate = nn.Sequential(nn.Linear(input_size + hidden_size, hidden_size), nn.Tanh())\n","\n","\n","\n","    def gru_step(self, x, h_prev):\n","        x_h = torch.cat((x,h_prev),dim=1)\n","        r = self.reset_gate(x_h)\n","        z = self.g_gate(x_h)\n","        x_rh = torch.cat((x, r * h_prev), dim=1)\n","        n = torch.tanh(x_rh) #Right??\n","        h = (1-z) * n + z*h_prev\n","        return h\n","\n","    #What exactly happen here? Do In still need this for loop?\n","    # Takes input tensor x with dimensions: [T, B, X].\n","    def forward(self, x, states):\n","        h = states\n","        outputs = []\n","        inputs = x.unbind(1)\n","        for x_t in inputs:\n","            print(f\"GRU Cell h size:{h.size()}, x size: {x_t.size()} \")\n","            h = self.gru_step(x_t, h)\n","            outputs.append(h)\n","        return torch.stack(outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.253408Z","start_time":"2024-07-16T22:07:57.222098Z"},"id":"vV42J7dDVwy3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LSTM_Cell(nn.Module):\n","    def __init__(self,\n","                 input_size,\n","                 hidden_size=200,\n","                 ):\n","        \"\"\"\n","        :param input_size:\n","        :param hidden_size:\n","        :param num_layers:\n","        :param num_classes:\n","        \"\"\"\n","        super(LSTM_Cell, self).__init__()\n","\n","        # size of the hidden state\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","        # LSTM gates\n","        # Forget gate\n","        self.f_gate = nn.Sequential(\n","            nn.Linear(input_size + hidden_size, hidden_size),\n","            nn.Sigmoid() )\n","\n","        # Candidate gate(input modulation in the original paper)\n","        self.g_gate = nn.Sequential(\n","            nn.Linear(input_size + hidden_size, hidden_size),\n","            nn.Tanh())\n","\n","        # Input gate\n","        self.i_gate = nn.Sequential(\n","            nn.Linear(input_size + hidden_size, hidden_size),\n","            nn.Sigmoid())\n","\n","        # Output gate\n","        self.o_gate = nn.Sequential(\n","            nn.Linear(input_size + hidden_size,hidden_size),\n","            nn.Sigmoid())\n","        self.tanh = nn.Tanh()\n","\n","    def lstm_step(self,x,h,c):\n","        \"\"\"\n","        :param x: input tensor\n","        :param h: previous hidden state\n","        :param c: previous cell state\n","        :return: (h,c) tuple of new cell state and new hidden state\n","        \"\"\"\n","        x, h, c = x.to(device), h.to(device), c.to(device)\n","        # Concatenate input and hidden state\n","        x_h = torch.cat((x,h),dim=1)\n","\n","        # Forget\n","        f = self.f_gate(x_h)\n","        g = self.g_gate(x_h)\n","        i = self.i_gate(x_h)\n","        o = self.o_gate(x_h)\n","\n","        # update c\n","        c = c * f + (g*i)\n","        # THEN, update h\n","        h = self.tanh(c) * o\n","\n","        # apply dropout to the hidden\n","\n","        return h,c\n","\n","    def forward(self, x, states):\n","        h, c = states\n","        outputs = []\n","        inputs = x.unbind(1)\n","\n","        #Run on words in sequence length\n","        for x_t in inputs:\n","            h, c = self.lstm_step(x_t, h, c)\n","\n","            outputs.append(h)\n","        output_seq_first = torch.stack(outputs) # shape is [seq, batch, embedd]\n","        output_batch_first= torch.transpose(output_seq_first, 0, 1) #shape is [batch, seq, embedd]\n","        return output_batch_first\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.254402Z","start_time":"2024-07-16T22:07:57.240919Z"},"id":"vnCnc7NgVwy4"},"outputs":[],"source":["class Network(nn.Module):\n","    def __init__(self,\n","                 input_size=input_size,\n","                 hidden_size=hidden_size,\n","                 num_layers=num_layers,\n","                 batch_size=batch_size,\n","                 vocab_size=vocab_size,\n","                 dropout=0, cell_type = \"lstm\"):\n","        \"\"\"\n","        :param input_size:\n","        :param hidden_size:\n","        :param num_layers:\n","        \"\"\"\n","\n","        super(Network, self).__init__()\n","\n","        self.embedding = nn.Sequential(\n","            nn.Embedding(vocab_size, input_size), #Dropout after embedding\n","            nn.Dropout(dropout)\n","        )\n","        self.cell_type = cell_type\n","        self.cell = LSTM_Cell if self.cell_type == \"lstm\" else GRU_Cell\n","        self.num_layers = num_layers\n","        self.batch_size = batch_size\n","        # implementing multilayer network\n","        self.rnns = nn.ModuleList([self.cell(input_size if i == 0 else hidden_size,\n","                                             hidden_size,\n","                                             dropout=dropout) for i in range(self.num_layers)])\n","        # self.rnns = [self.cell(input_size,hidden_size,dropout=dropout) for i in range(self.num_layers)]\n","        self.hidden_size = hidden_size\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","\n","    def forward_step(self,x):\n","         if self.cell_type == \"lstm\":\n","            return self.forward_lstm(x)\n","         else:\n","            return self.forward_gru(x)\n","\n","\n","    def forward(self,x):\n","            \"\"\"\n","            :param x: input tensor\n","            :return: (c,h) tuple of new cell state and new hidden state\n","            \"\"\"\n","            outputs = []\n","            batch_size = x.size(0)\n","\n","            if (self.cell_type == 'lstm'):\n","                #initial c, h for all LSTM Layers\n","                states=[(torch.zeros(self.batch_size, self.hidden_size), torch.zeros(self.batch_size, self.hidden_size)) for layer in self.rnns]\n","\n","            elif (self.cell_type == 'gru'):\n","            #intialize only h for GRU laters\n","             states = [(torch.zeros(self.batch_size, self.hidden_size)) for layer in self.rnns]\n","            else:\n","             print(\"Cell Type : {:3f} is not Valid\".format(self.cell_type))\n","\n","            # store the hidden states (output)\n","            x = self.embedding(x) #shape is [batch, seq, embedd]\n","\n","\n","            for i, rnn in enumerate(self.rnns): #run over all layers\n","              x = rnn(x, states[i]) #shape of X is [batch, embedd]\n","              x = self.dropout(x) #Dropout between rnn's layers\n","            output = self.fc(x) # size is batch, seq_length, vocab_size\n","\n","            return output\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.267407Z","start_time":"2024-07-16T22:07:57.246399Z"},"id":"LJamjuE2Vwy5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:07:57.302812Z","start_time":"2024-07-16T22:07:57.265363Z"},"id":"nVCtxkaqVwy5"},"outputs":[],"source":["def evaluate(model,\n","             val_loader,\n","             criterion = nn.CrossEntropyLoss(),\n","             seq_length = 32):\n","    \"\"\"\n","    evaluate the model on the validation set\n","    :param model: model to evaluate\n","    :param val_loader: validation dataset loader\n","    :return: (accuracy, loss)\n","    \"\"\"\n","    model.eval()\n","    model.to(device)\n","    running_loss = 0.0\n","    running_acc = 0\n","    total = 0\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader, 0):\n","            inputs, targets = data\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = model(inputs)\n","\n","            outputs = outputs.view(-1, vocab_size)\n","            targets = targets.view(-1)\n","\n","            loss = criterion(outputs, targets)\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            running_acc += (predicted == targets).sum().item()\n","            total += targets.size(0)\n","        avg_loss = running_loss/ len(val_loader)\n","        perplexity = np.exp(avg_loss)\n","    return perplexity, loss"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"FDFTCtnYVwy6"},"source":["# Example"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:08:06.816190Z","start_time":"2024-07-16T22:07:57.278745Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3786,"status":"ok","timestamp":1721218937454,"user":{"displayName":"Reut Salman","userId":"06615170649265185928"},"user_tz":-180},"id":"sDick2i9Vwy7","outputId":"73171a75-90fd-4ac1-a8e8-892691148964"},"outputs":[],"source":["x,y = next(train_data_iter)\n","print(x.size())\n","lstm = Network(200, 200, 1)\n","\n","evaluate(lstm, valid_loader)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-17T10:52:14.093848Z","start_time":"2024-07-17T10:52:14.037426Z"},"id":"zJpd3OANVwy7"},"outputs":[],"source":["\n","\n","def train(model,\n","          criterion,\n","          optimizer,\n","          train_loader=train_loader,\n","          val_loader=valid_loader,\n","          epochs=35,\n","          session = None,\n","          start_epoch = 0,\n","          scheduler = None,\n","          ):\n","    #create /models directory IF it does not exist\n","    if not os.path.exists('./models'):\n","        # Create the directory\n","        print(\"Creating models directory\")\n","        os.makedirs('./models')\n","\n","    # track with tensorboard\n","    session = session or 'LSTM'+datetime.now().strftime('%m-%d-%H-%M')\n","    #tb_writer = SummaryWriter(f'runs/{session}')\n","    run_dir = f'{log_dir}/{session}'\n","    print(run_dir)\n","    tb_writer = SummaryWriter(run_dir)\n","    tb_writer.flush()\n","\n","    val_loss_min = np.Inf\n","    model.train()\n","    model.to(device)\n","    for epoch in range(start_epoch, start_epoch + epochs):\n","        print(\"Epoch : {:d} out of {:d}\".format(epoch, epochs))\n","        running_loss = 0.0\n","        running_corrects = 0\n","        train_total = 0\n","\n","        # set the model to train mode\n","        model.train(True)\n","\n","        for i, data in enumerate(train_loader, 0):\n","            # get the input image and labels\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            # start with zero gradients\n","            optimizer.zero_grad()\n","            # forward pass\n","            outputs = model(inputs)\n","\n","            # reshape the outputs and labels (batch_size , seq_length, vocab_size)=>(batch_size * seq_length, vocab_size)\n","            outputs = outputs.view(-1, vocab_size)\n","            labels = labels.view(-1)\n","\n","            # calculate the loss\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            # update the weights\n","            optimizer.step()\n","            running_loss += loss.item()\n","            _, preds = torch.max(outputs, 1)\n","\n","            running_corrects += (preds == labels).sum().item()\n","            train_total += labels.size(0)\n","\n","            if i % 100 == 99:\n","                # print gradient statistics\n","                for name, param in model.named_parameters():\n","                    tb_writer.add_histogram(name, param.grad, epoch * len(train_loader) +  i)\n","                # loss of current batch\n","                avg_train_loss = running_loss / 100\n","                print(\"batch no = {:d} / {:d}, \".format(i, len(train_loader)) +\n","                      \"Avrage train loss = {:.3f}, \".format(avg_train_loss) +\n","                      \"lr = {:.3f}, \".format(lr) +\n","                      \"cuda memory = {:.3f} GBs\".format(torch.cuda.max_memory_allocated()/1024/1024/1024))\n","                tb_writer.add_scalar('training loss', avg_train_loss, epoch * len(train_loader) +  i)\n","\n","\n","                running_loss = 0.0\n","\n","        \"\"\"-----------------\n","        per epoch evaluation\n","        -----------------\"\"\"\n","        # set the model to evaluation mode\n","        model.eval()\n","\n","        # TODO convert accuracy to perplexity\n","        # TODO: maybe move perplexity tensorboard log to happen more often\n","        train_perplexity = np.exp(avg_train_loss)\n","        # validation\n","        val_perplexity, val_loss = evaluate(model, val_loader)\n","\n","        if scheduler:\n","            scheduler.step()\n","            print(\"Learning rate: \", optimizer.param_groups[0]['lr'])\n","\n","        #val_accuracy = (val_preds == val_labels).sum().item() / len(val_loader)\n","        tb_writer.add_scalars('train vs val loss', {'train': avg_train_loss, 'val': val_loss}, epoch)\n","        tb_writer.add_scalars('train vs val perplexity', {'train': train_perplexity, 'val': val_perplexity}, epoch)\n","        print(f'at epoch {epoch}: \\nvalidation loss: {val_loss} \\nValidation Preplexity: {val_perplexity}\\ntraining loss:   {avg_train_loss} ')\n","        tb_writer.add_scalar('validation loss', val_loss, epoch)\n","        if val_loss <= val_loss_min:\n","            print('validation loss decreased({:.6f} -->{:.6f}). Saving Model ...'.format(val_loss_min, val_loss))\n","            torch.save(model, f'./models/ {session}.pt')\n","            val_loss_min = val_loss\n","    print('Finished Training')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"f4P90tyOVwy8"},"source":["# Train the model\n","LSTM no drop out"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-16T22:08:06.905031Z","start_time":"2024-07-16T22:08:06.855941Z"},"id":"b5-woBgYVwy8"},"outputs":[],"source":["\n","\n","lstm = Network(input_size=input_size,\n","               hidden_size=hidden_size,\n","               num_layers=num_layers,\n","                vocab_size=vocab_size,\n","                dropout=0,\n","               cell_type = \"lstm\" )\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-JP5rsLzVwy8","outputId":"75a54943-e413-45af-88fb-8991ca70df40"},"outputs":[],"source":["optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.2)\n","\n","epochs =35\n","lstm.to(device)\n","train(lstm,\n","      criterion = criterion,\n","      optimizer=optimizer,\n","      train_loader=train_loader,\n","      val_loader=valid_loader,\n","      session='vanillaLSTM',\n","      start_epoch=1,\n","      scheduler = scheduler\n","      )\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"742T8dDsBv0s"},"source":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xyLWkwyIBv0x"},"source":["## Training LSTM with Dropout"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"_j6R-qOYBv0x"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dq9NYidLBv0x"},"outputs":[],"source":["lstm = Network(input_size=input_size,\n","               hidden_size=hidden_size,\n","               num_layers=num_layers,\n","               vocab_size=vocab_size,\n","               dropout=0.5,\n","               cell_type = \"lstm\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H82jSVFuBv0y","outputId":"1caba10e-9ead-4865-e527-8f6401fb5859","pycharm":{"is_executing":true}},"outputs":[],"source":["optimizer = torch.optim.Adam(lstm.parameters(), lr=0.1, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2, verbose=True)\n","epochs =35\n","\n","train(lstm,\n","      epochs=epochs,\n","      criterion = criterion,\n","      optimizer=optimizer,\n","      train_loader=train_loader,\n","      val_loader=valid_loader,\n","      session='dropoutLSTM',\n","      start_epoch=1,\n","      scheduler = scheduler\n","\n","      )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kI86ZirZVwy9"},"outputs":[],"source":["\n","# Create dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","\n","lstm = LSTM(input_size=200,\n","            hidden_size=200,\n","            num_layers=1,\n","            vocab_size=vocab_size,\n","            dropout=0)\n","\n","optimizer = torch.optim.Adam(lstm.parameters(), lr=lr)\n","train(lstm,\n","      session='vanilla LSTM no l2 reg',\n","      optimizer=optimizer,\n","      val_loader=valid_loader,\n","      train_loader=train_loader,\n","      epochs=20\n","      )"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"De3ETtq2VwzK"},"source":["# GRU Train with Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-07-09T21:19:33.227136Z","start_time":"2024-07-09T19:24:39.612044Z"},"id":"T-AIh39DVw0c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZhRuv-UVw0d"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
